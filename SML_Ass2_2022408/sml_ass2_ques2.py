# -*- coding: utf-8 -*-
"""SML_Ass2_Ques2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fu82ZGKlv3VS12-AP0a3g57fHJe2-Svg
"""

import numpy as np
import math
import matplotlib.pyplot as plt
import tensorflow as tf
import random

# from question 1
# made use of pseudo determinant for the qda function instead of epsilon like in question1
class mnistClassifier:
    def __init__(self, train, train_label, epsilon, cov_divisor):
        self.train = train
        self.train_label = train_label

        # Initialize matrices and counts for each class
        self.class_matrix = []
        self.count = np.zeros(10, dtype=int)

        # Separate images by class
        for i in range(10):
            vector = []
            for j in range(len(train)):
                self.count[i] += 1 if train_label[j] == i else 0
                if train_label[j] == i:
                    vector.append(train[j])
            self.class_matrix.append(vector)

        # Convert class matrices to numpy arrays
        for i in range(10):
            self.class_matrix[i] = np.array(self.class_matrix[i])
        self.count = np.array(self.count)

        # for i in range(10):
        #     print(self.class_matrix[i].shape)

        self.classDataVect = []

        # vectorise the data
        for i in range(10):
            self.classDataVect.append(np.array(self.class_matrix[i].reshape(self.count[i], train.shape[1])))
            self.classDataVect[i] = np.array(self.classDataVect[i])

        # for i in range(10):
        #     print(self.classDataVect[i].shape)

        # calculating the mean of all the classes
        self.mean = []
        for i in range(10):
            self.mean.append(np.mean(self.classDataVect[i], axis=0))
        self.mean = np.array(self.mean)


        # calculating the covariance matrices of all the classes
        self.cov = []
        self.epsilon = epsilon     # Regularization term to avoid singularity
        self.divisor = cov_divisor  # Divisor for the covariance matrix
        for i in range(10):
            cov_matrix = np.cov(self.classDataVect[i].T) / self.divisor
            # print(max(cov_matrix.flatten()), min(cov_matrix.flatten()))
            cov_matrix += epsilon * np.identity(cov_matrix.shape[0])  # Add regularization term
            self.cov.append(cov_matrix)

        self.cov = np.array(self.cov)

        # calculating the unregularized covariance matrices
        self.cov_unreg = []
        for i in range(10):
            self.cov_unreg.append(np.cov(self.classDataVect[i].T))
        self.cov_unreg = np.array(self.cov_unreg)

        # calculating the determinant of the covariance matrices
        cov_det = []
        for i in range(10):
            cov_det.append(np.linalg.slogdet(self.cov[i]))

        # calculating the log of psuedo determinant of the covariance matrices using the eigen values
        psuedo_det = []
        for i in range(10):
            eigvalues = np.real(np.linalg.eigvals(self.cov_unreg[i]));
            pseudo_determinent = np.sum(np.log(eigvalues[eigvalues > 1e-12]))
            psuedo_det.append(pseudo_determinent)
        # self.log_pcov_det = np.log(np.array(psuedo_det))
        self.log_pcov_det = np.array(psuedo_det)
        # print(self.log_pcov_det)

        log_det = []
        for i in range(10):
            log_det.append(cov_det[i][1])
        self.log_cov_det = np.array(log_det)

        # calculating the inverse of the covariance matrices
        self.cov_inv = []
        for i in range(10):
            self.cov_inv.append(np.linalg.inv(self.cov[i]))
        self.cov_inv = np.array(self.cov_inv)

        # calculating the psuedo inverse of the covariance matrices
        self.cov_pinv = []
        for i in range(10):
            self.cov_pinv.append(np.linalg.pinv(self.cov_unreg[i]))
        self.cov_pinv = np.array(self.cov_pinv)

    def categorise(self, test, regularise):
        prob = []
        # calculating the probability of each class with regularisation
        if regularise:
            for i in range(10):
                # calculating the discriminant function of each class using QDA
                prob.append(math.log(self.count[i]/len(self.train)) - 0.5 * self.log_cov_det[i] -
                            0.5 * np.dot(np.dot((np.subtract(test, self.mean[i])), self.cov_inv[i]), (np.subtract(test, self.mean[i]).T)))
            prob = np.array(prob)
            prob = prob.reshape(10, 1)

        # calculating the probaility of each class without regularisation
        else:
            for i in range(10):
                # calculating the discriminant function of each class using QDA
                prob.append(math.log(self.count[i]/len(self.train)) - 0.5 * self.log_pcov_det[i] -
                            0.5 * np.dot(np.dot((np.subtract(test, self.mean[i])), self.cov_pinv[i]), (np.subtract(test, self.mean[i]).T)))
            prob = np.array(prob)
            prob = prob.reshape(10, 1)

        # finding the class with maximum probability
        return np.argmax(prob)

    def accuracy(self, test, test_label, regularise):
        count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        correct_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        for i in range(len(test)):
            count[test_label[i]] += 1
            if self.categorise(test[i], regularise) == test_label[i]:
                correct_count[test_label[i]] += 1

        accuracy = []
        print("\nAccuracy of each class")
        for i in range(10):
            print(i, "-->",  round((correct_count[i]/count[i])*100, 3), end=" %\n")
            accuracy.append(round((correct_count[i]/count[i])*100, 3))
        print("\nOverall Accuracy")
        print("  ",round((sum(correct_count)/sum(count))*100,3), end=" %\n")

        return round((sum(correct_count)/sum(count))*100,3), accuracy

def calculate_mean(X):
    X_mean = np.mean(X, axis=1).reshape(784, 1)
    return X_mean

def calculate_covariance(X):
    cov_X = np.dot(X, X.T) / 999
    return cov_X

def calculate_eigen(cov_X):
    eigen_values, eigen_vectors = np.linalg.eig(cov_X)
    eigen_values = np.real(eigen_values)
    eigen_vectors = np.real(eigen_vectors)
    idx = eigen_values.argsort()[::-1]
    eigen_values = eigen_values[idx]
    eigen_vectors = eigen_vectors[:, idx]
    return eigen_values, eigen_vectors

def generate_U(eigen_vectors):
    U = [eigen_vectors[:, :i+1] for i in range(len(eigen_vectors))]
    return U

def plot_X_recon(U, X, X_mean, p):
    U_p = U[p - 1]
    Y = np.dot(U_p.T, X)
    X_recon = np.dot(U_p, Y)
    X_recon = X_recon + X_mean
    X_recon = X_recon.T.reshape(1000, 28, 28)
    X_recon_label = np.zeros(1000, dtype=int)
    for i in range(10):
        for j in range(100):
            X_recon_label[i*100 + j] = i
    fig, ax = plt.subplots(10, 5, figsize=(10, 10))
    for i in range(10):
        for j in range(5):
            random_index = random.randint(0, 19)
            ax[i, j].imshow(X_recon[i*100 + j*random_index], cmap='Purples')
            ax[i, j].set_title("Class " + str(i))
            ax[i, j].axis('off')
    plt.suptitle(f'Reconstructed images with p={p} principal components')
    plt.tight_layout()
    plt.show()
    X_recon = X_recon.reshape(1000, 784)
    X_recon = X_recon.T
    X_recon = X_recon - X_mean
    mse = np.mean((X - X_recon) ** 2)
    print("MSE: ", mse)
    return X_recon, X_recon_label

def plot_eigen_values(eigen_values):
    plt.figure()
    plt.plot(eigen_values[:100])
    plt.title("Screen Plot")
    plt.xlabel("Principal Components")
    plt.ylabel("Eigen Values")
    plt.show()

def check_with_test(U, test, test_label, train, train_label, p):
    test = test.reshape(len(test), 784).T
    train = train.reshape(len(train), 784).T
    U_p = U[p - 1]
    Y_Train = np.dot(U_p.T, train)
    new_classifier = mnistClassifier(Y_Train.T, train_label, 1e-6, 1)
    Y = np.dot(U_p.T, test)
    return new_classifier.accuracy(Y.T, test_label, regularise=False)

def preprocess_data(train, train_label, test, test_label):
    train = train.reshape(len(train), 784)
    test = test.reshape(len(test), 784)
    class_matrix = []
    count = np.zeros(10, dtype=int)
    for i in range(10):
        vector = []
        for j in range(len(train)):
            count[i] += 1 if train_label[j] == i else 0
            if train_label[j] == i:
                vector.append(train[j])
        class_matrix.append(vector)
    for i in range(10):
        class_matrix[i] = np.array(class_matrix[i])
    count = np.array(count)
    classDataVect = []
    for i in range(10):
        classDataVect.append(np.array(class_matrix[i].reshape(count[i], train.shape[1])))
        classDataVect[i] = np.array(classDataVect[i])
    return classDataVect, count

def mnist_generator(classDataVect, count):
    X = []
    for i in range(10):
        for j in random.sample(range(count[i]), 100):
            X.append(classDataVect[i][j])
    X = np.array(X).T
    X_mean = calculate_mean(X)
    X = X - X_mean
    cov_X = calculate_covariance(X)
    eigen_values, eigen_vectors = calculate_eigen(cov_X)
    U = generate_U(eigen_vectors)
    Y = np.dot(U[-1].T, X)
    X_recon = np.dot(U[-1], Y)
    X_recon = X_recon + X_mean
    X_recon = X_recon.T
    X_recon = X_recon.reshape(1000, 28, 28)
    X_recon_label = np.zeros(1000, dtype=int)
    for i in range(10):
        for j in range(100):
            X_recon_label[i*100 + j] = i
    X_recon = X_recon.reshape(1000, 784)
    X_recon = X_recon.T
    X_recon = X_recon - X_mean
    mse = np.mean((X - X_recon) ** 2)
    print("MSE: ", mse)
    return X_recon, X_recon_label, U, X_mean, eigen_values

def main():
    ## Loading the MNIST dataset
    mnist = tf.keras.datasets.mnist
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()
    classes = np.unique(train_labels)
    plt.figure(figsize=(12, 10))
    for i, cls in enumerate(classes):
        idxs = np.where(train_labels == cls)[0][:5]
        for j, idx in enumerate(idxs):
            plt.subplot(len(classes), 5, i * 5 + j + 1)
            plt.imshow(train_images[idx], cmap='Purples')
            plt.title(f'Class {cls}')
            plt.axis('off')
    plt.show()  # Displaying the dataset

    classDataVect, count = preprocess_data(train_images, train_labels, test_images, test_labels)
    X_recon, X_recon_label, U, X_mean, eigen_values = mnist_generator(classDataVect, count)
    print("p = 5")
    plot_X_recon(U, X_recon, X_mean, 5)
    check_with_test(U, test_images, test_labels, train_images, train_labels, 5)
    print("p = 10")
    plot_X_recon(U, X_recon, X_mean, 10)
    check_with_test(U, test_images, test_labels, train_images, train_labels, 10)
    print("p = 20")
    plot_X_recon(U, X_recon, X_mean, 20)
    check_with_test(U, test_images, test_labels, train_images, train_labels, 20)
    print("p = 40")
    plot_X_recon(U, X_recon, X_mean, 40)
    check_with_test(U, test_images, test_labels, train_images, train_labels, 40)
    print("p = 784")
    plot_X_recon(U, X_recon, X_mean, 784)
    check_with_test(U, test_images, test_labels, train_images, train_labels, 784)
    plot_eigen_values(eigen_values)


    # check_with_test(U, test_images, test_labels, train_images, train_labels, 200)

    # p_values = [5, 10, 20, 40, 784]
    # accuracies = []

    # for p in p_values:
    #     _, _, U, _, _ = mnist_generator(classDataVect, count)
    #     accuracy = check_with_test(U, test_images, test_labels, train_images, train_labels, p)
    #     accuracies.append(accuracy)

    # plt.plot(p_values, accuracies)
    # plt.xlabel('Number of Principal Components (p)')
    # plt.ylabel('Accuracy')
    # plt.title('Accuracy vs Number of Principal Components')
    # plt.show()

     # ----------------------- for plotting the accuracy of each class and overall-----------------------
    eigens = [5, 10, 20, 40, 784]
    accuracies = []
    class_accuracies = []
    for eigen in eigens:
        accuracy, class_accuracy = check_with_test(U, test_images, test_labels, train_images, train_labels, eigen)
        accuracies.append(accuracy)
        class_accuracies.append(class_accuracy)
    class_accuracies = np.array(class_accuracies)
    class_accuracies = class_accuracies.T


    plt.figure()
    plt.plot(eigens, accuracies)
    plt.title("Overall Accuracy vs No of Eigen Values")
    plt.xlabel("P")
    plt.ylabel("Accuracy")
    plt.show()

    plt.figure()
    for i in range(10):
        plt.plot(eigens, class_accuracies[i], label="Class " + str(i))
    plt.title("Classwise Accuracy vs No of Eigen Values")
    plt.xlabel("P")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()
if __name__ == "__main__":
    main()

